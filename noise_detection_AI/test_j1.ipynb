{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import librosa\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/ESC-50'\n",
    "path = os.path.join(base_path, 'meta/esc50.csv')\n",
    "csv_input = pd.read_csv(path, encoding=\"ms932\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation: add white noise\n",
    "def add_white_noise(x, rate=0.002):\n",
    "    return x + rate*np.random.randn(len(x))\n",
    "\n",
    "\n",
    "# data augmentation: shift sound in timeframe\n",
    "def shift_sound(x, rate=2):\n",
    "    return np.roll(x, int(len(x)//rate))\n",
    "\n",
    "\n",
    "# data augmentation: stretch sound\n",
    "def stretch_sound(x, rate=1.1):\n",
    "    input_length = len(x)\n",
    "    x = librosa.effects.time_stretch(x, rate)\n",
    "    if len(x) > input_length:\n",
    "        return x[:input_length]\n",
    "    else:\n",
    "        return np.pad(x, (0, max(0, input_length - len(x))), \"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.array([])\n",
    "labels = np.array([])\n",
    "\n",
    "for i in range(2000):\n",
    "    if csv_input.category[i] == 'snoring':\n",
    "        path = np.append(path, os.path.join(base_path, 'audio/' + csv_input.filename[i]))\n",
    "        for _ in range(4):\n",
    "            labels = np.append(labels, 'snoring')\n",
    "    elif csv_input.category[i] == 'washing_machine':\n",
    "        path = np.append(path, os.path.join(base_path, 'audio/' + csv_input.filename[i]))\n",
    "        for _ in range(4):\n",
    "            labels = np.append(labels, 'washing_machine')\n",
    "    elif csv_input.category[i] == 'vacuum_cleaner':\n",
    "        path = np.append(path, os.path.join(base_path, 'audio/' + csv_input.filename[i]))\n",
    "        for _ in range(4):\n",
    "            labels = np.append(labels, 'vacuum_cleaner')\n",
    "    elif csv_input.category[i] == 'helicopter':\n",
    "        path = np.append(path, os.path.join(base_path, 'audio/' + csv_input.filename[i]))\n",
    "        for _ in range(4):\n",
    "            labels = np.append(labels, 'helicopter')\n",
    "    elif csv_input.category[i] == 'chainsaw':\n",
    "        path = np.append(path, os.path.join(base_path, 'audio/' + csv_input.filename[i]))\n",
    "        for _ in range(4):\n",
    "            labels = np.append(labels, 'chainsaw')\n",
    "    elif csv_input.category[i] == 'engine':\n",
    "        path = np.append(path, os.path.join(base_path, 'audio/' + csv_input.filename[i]))\n",
    "        for _ in range(4):\n",
    "            labels = np.append(labels, 'engine')\n",
    "    elif csv_input.category[i] == 'airplane':\n",
    "        path = np.append(path, os.path.join(base_path, 'audio/' + csv_input.filename[i]))\n",
    "        for _ in range(4):\n",
    "            labels = np.append(labels, 'airplane')\n",
    "    else:\n",
    "        path = np.append(path, os.path.join(base_path, 'audio/' + csv_input.filename[i]))\n",
    "        for _ in range(4):\n",
    "            labels = np.append(labels, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "melspecs = []\n",
    "mfccs = []\n",
    "two_sec = 88200 # 2sec\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    y_1, sr = librosa.load(path[i], sr = 44100)\n",
    "    y_1, _ = librosa.effects.trim(y_1, top_db=20)\n",
    "\n",
    "    if len(y_1) <= two_sec:\n",
    "        # padding\n",
    "        pad =  two_sec - len(y_1)\n",
    "        y_1 = np.concatenate((y_1, np.zeros(pad, dtype=np.float16)))\n",
    "    else:\n",
    "        # trimming\n",
    "        start = random.randint(0, len(y_1) -  two_sec - 1)\n",
    "        y_1 = y_1[start:start +  two_sec]\n",
    "\n",
    "    y_2 = add_white_noise(y_1)\n",
    "    y_3 = shift_sound(y_1)\n",
    "    y_4 = stretch_sound(y_1)\n",
    "\n",
    "    dataset.append(y_1)\n",
    "    dataset.append(y_2)\n",
    "    dataset.append(y_3)\n",
    "    dataset.append(y_4)\n",
    "\n",
    "    # メルスペクトログラムの計算\n",
    "    melspec_1 = librosa.feature.melspectrogram(y_1, sr)\n",
    "    melspec_2 = librosa.feature.melspectrogram(y_2, sr)\n",
    "    melspec_3 = librosa.feature.melspectrogram(y_3, sr)\n",
    "    melspec_4 = librosa.feature.melspectrogram(y_4, sr)\n",
    "    melspec_1 = librosa.amplitude_to_db(melspec_1).flatten()\n",
    "    melspec_2 = librosa.amplitude_to_db(melspec_2).flatten()\n",
    "    melspec_3 = librosa.amplitude_to_db(melspec_3).flatten()\n",
    "    melspec_4 = librosa.amplitude_to_db(melspec_4).flatten()\n",
    "\n",
    "    melspecs.append(melspec_1.astype(np.float16))\n",
    "    melspecs.append(melspec_2.astype(np.float16))\n",
    "    melspecs.append(melspec_3.astype(np.float16))\n",
    "    melspecs.append(melspec_4.astype(np.float16))\n",
    "\n",
    "    mfcc_1 = np.mean(librosa.feature.mfcc(y=y_1, sr=sr, n_mfcc=40), axis=1)\n",
    "    mfcc_2 = np.mean(librosa.feature.mfcc(y=y_2, sr=sr, n_mfcc=40), axis=1)\n",
    "    mfcc_3 = np.mean(librosa.feature.mfcc(y=y_3, sr=sr, n_mfcc=40), axis=1)\n",
    "    mfcc_4 = np.mean(librosa.feature.mfcc(y=y_4, sr=sr, n_mfcc=40), axis=1)\n",
    "\n",
    "    mfccs.append(mfcc_1)\n",
    "    mfccs.append(mfcc_2)\n",
    "    mfccs.append(mfcc_3)\n",
    "    mfccs.append(mfcc_4)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=\",\")\n",
    "print(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset)\n",
    "melspecs = np.array(melspecs)\n",
    "mfccs = np.array(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "# 各データの振幅の平均値\n",
    "mean = np.sqrt(np.mean(dataset**2, axis=1))\n",
    "print(len(mean))\n",
    "\n",
    "# 各データのゼロクロス数\n",
    "zc = np.sum(librosa.zero_crossings(dataset), axis=1)\n",
    "print(len(zc))\n",
    "\n",
    "# train_feature学習データ，test_feature予測（テスト）データ\n",
    "train_feature = np.array([])\n",
    "train_feature = [np.append(np.append(np.append(np.append(train_feature, mean[i]), zc[i]), melspecs[i]), mfccs[i]) for i in range(8000)]\n",
    "print(len(train_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pr0gr\\RD_club\\projects\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_feature, labels, test_size=0.2, random_state=0, stratify=labels)\n",
    "model_1 = LinearSVC()\n",
    "model_1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1(train): 0.99703125\n",
      "model_1(test): 0.905\n"
     ]
    }
   ],
   "source": [
    "print(f\"model_1(train): {model_1.score(x_train, y_train)}\")\n",
    "print(f\"model_1(test): {model_1.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pr0gr\\RD_club\\projects\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(train_feature, labels, test_size=0.1, random_state=0, stratify=labels)\n",
    "model_2 = LinearSVC()\n",
    "model_2.fit(x_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_2(train): 0.99875\n",
      "model_2(test): 0.94375\n"
     ]
    }
   ],
   "source": [
    "print(f\"model_2(train): {model_2.score(x_train_2, y_train_2)}\")\n",
    "print(f\"model_2(test): {model_2.score(x_test_2, y_test_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "#条件設定\n",
    "max_score = 0\n",
    "SearchMethod = 0\n",
    "LSVC_grid = {LinearSVC(): {\"C\": [10 ** i for i in range(-5, 6)],\n",
    "                           \"multi_class\": [\"ovr\", \"crammer_singer\"],\n",
    "                           \"class_weight\": [\"balanced\"],\n",
    "                           \"random_state\": [i for i in range(0, 101)]}}\n",
    "LSVC_random = {LinearSVC(): {\"C\": scipy.stats.uniform(0.00001, 1000),\n",
    "                             \"multi_class\": [\"ovr\", \"crammer_singer\"],\n",
    "                             \"class_weight\": [\"balanced\"],\n",
    "                             \"random_state\": scipy.stats.randint(0, 100)}}\n",
    "\n",
    "#トレーニングデータ、テストデータの分離\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_feature, labels, random_state=0)\n",
    "\n",
    "#グリッドサーチ\n",
    "for model, param in LSVC_grid.items():\n",
    "    clf = GridSearchCV(model, param)\n",
    "    clf.fit(train_X, train_y)\n",
    "    pred_y = clf.predict(test_X)\n",
    "    score = f1_score(test_y, pred_y, average=\"micro\")\n",
    "\n",
    "    if max_score < score:\n",
    "        max_score = score\n",
    "        best_param = clf.best_params_\n",
    "        best_model = model.__class__.__name__\n",
    "\n",
    "#ランダムサーチ\n",
    "for model, param in LSVC_random.items():\n",
    "    clf =RandomizedSearchCV(model, param)\n",
    "    clf.fit(train_X, train_y)\n",
    "    pred_y = clf.predict(test_X)\n",
    "    score = f1_score(test_y, pred_y, average=\"micro\")\n",
    "\n",
    "    if max_score < score:\n",
    "        SearchMethod = 1\n",
    "        max_score = score\n",
    "        best_param = clf.best_params_\n",
    "        best_model = model.__class__.__name__\n",
    "\n",
    "if SearchMethod == 0:\n",
    "    print(\"サーチ方法:グリッドサーチ\")\n",
    "else:\n",
    "    print(\"サーチ方法:ランダムサーチ\")\n",
    "print(\"ベストスコア:{}\".format(max_score))\n",
    "print(\"モデル:{}\".format(best_model))\n",
    "print(\"パラメーター:{}\".format(best_param))\n",
    "\n",
    "#ハイパーパラメータを調整しない場合との比較\n",
    "model = LinearSVC()\n",
    "model.fit(train_X, train_y)\n",
    "score = model.score(test_X, test_y)\n",
    "print(\"\")\n",
    "print(\"デフォルトスコア:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_base_path = '/G2021-maikataA_py'\n",
    "save_path = os.path.join(C_base_path, 'noise_detection_AI/pickle')\n",
    "with open(save_path + '/all_model_0.joblib', mode='wb') as f:\n",
    "    joblib.dump(model_2, f, protocol = 4)\n",
    "\n",
    "save_path = os.path.join(C_base_path, 'save_AI')\n",
    "with open(save_path + '/model_predict.joblib', mode='wb') as f:\n",
    "    joblib.dump(model_2, f, protocol = 4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79d881d3545764ae37b90ddefa6c13335ad6bd58da30226de060c17cdaddbf42"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
